{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dd8f2136",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "\n",
    "import abc\n",
    "from abc import ABC\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from enum import Enum\n",
    "from tqdm import tqdm\n",
    "from typing import Any, Dict, List, Optional, Tuple, Set, Union, cast\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from datasets import Dataset, load_dataset\n",
    "from lightning import Fabric\n",
    "from lightning.fabric.strategies import DDPStrategy\n",
    "from lightning.pytorch.callbacks import DeviceStatsMonitor, LearningRateMonitor\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from transformers.models.bert.modeling_bert import BertLayer\n",
    "from transformers.models.bert import BertForSequenceClassification\n",
    "from transformers import AutoTokenizer, BertTokenizer, PreTrainedTokenizer\n",
    "\n",
    "from _common import RESULTS_DIR, logging\n",
    "from fusionlib.merge.task_arithmetic import task_arithmetic_merge_modules\n",
    "from src.module.dict_moe import ParetoWeightEnsemblingModule\n",
    "from src.module.utils import print_trainable_parameters\n",
    "from src.phn.solvers import EPOSolver\n",
    "from src.utils import timeit_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "76d8dc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure log.\n",
    "log = logging.getLogger(__name__)\n",
    "log.setLevel(logging.DEBUG)\n",
    "handler = logging.StreamHandler()\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
    "handler.setFormatter(formatter)\n",
    "log.addHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b1a58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = DictConfig(\n",
    "    {\n",
    "        \"model\": \"bert-base-uncased\",\n",
    "        \"version\": None,\n",
    "        \"num_devices\": 1,\n",
    "        \"tasks\": {\"task1\": 1, \"task2\": 1},\n",
    "        \"partial\": True,\n",
    "        \"init_lambda\": 0.6,\n",
    "        \"router_hidden_layers\": 1,\n",
    "        \"batch_size\": 1,\n",
    "        \"train\": True,\n",
    "        \"lr\": 1e-2,\n",
    "        \"num_steps\": 1000,\n",
    "        \"alpha\": 1,\n",
    "        \"save_interval\": 500,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "30697a3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('c:/Users/Admin/OneDrive/Documents/DANC/source_code/pareto_set_learning/results')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RESULTS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c231aadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(cfg.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2e0be069",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mode(Enum):\n",
    "    train = \"train\"\n",
    "    dev = \"dev\"\n",
    "    test = \"test\"\n",
    "    inference = \"inference\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a90c513b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 22:50:22,698 - __main__ - INFO - Load IMDB dataset from Mode.train split\n",
      "2025-04-15 22:50:22,698 - __main__ - INFO - Load IMDB dataset from Mode.train split\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:01<00:00, 23733.73it/s]\n",
      "2025-04-15 22:50:37,584 - __main__ - INFO - Loaded dataset with 2 labels\n",
      "2025-04-15 22:50:37,584 - __main__ - INFO - Loaded dataset with 2 labels\n",
      "2025-04-15 22:50:37,590 - __main__ - INFO - Load IMDB dataset from Mode.train split\n",
      "2025-04-15 22:50:37,590 - __main__ - INFO - Load IMDB dataset from Mode.train split\n",
      "100%|██████████| 25000/25000 [00:00<00:00, 26374.38it/s]\n",
      "2025-04-15 22:50:50,663 - __main__ - INFO - Loaded dataset with 2 labels\n",
      "2025-04-15 22:50:50,663 - __main__ - INFO - Loaded dataset with 2 labels\n"
     ]
    }
   ],
   "source": [
    "def load_imdb_dataset(split: Mode) -> Tuple[Dict[str, List[str]], List[str]]:\n",
    "    \"\"\"\n",
    "    Load IMDB dataset.\n",
    "    :param split: Train or Test split.\n",
    "    :return: Dataset in dictionary format and list of all labels of the dataset.\n",
    "    \"\"\"\n",
    "    log.info(f\"Load IMDB dataset from {split} split\")\n",
    "    # TODO: We convert the dataset into format {\"text\": [], \"labels\": []} which was used commonly. This step can require\n",
    "    #  a large MEM as the dataset is duplicated.\n",
    "    imdb_dataset = load_dataset(\"imdb\")[split.value]\n",
    "    output_dataset: Dict[str, List[str]] = {\"text\": [], \"labels\": []}\n",
    "    all_labels: Set[str] = set()\n",
    "    for sample in tqdm(imdb_dataset):\n",
    "        output_dataset[\"text\"].append(sample[\"text\"])\n",
    "        label = str(sample[\"label\"])\n",
    "        output_dataset[\"labels\"].append(label)\n",
    "        all_labels.add(label)\n",
    "    log.info(f\"Loaded dataset with {len(all_labels)} labels\")\n",
    "    return output_dataset, list(all_labels)\n",
    "\n",
    "\n",
    "dataset_mapping = {\n",
    "    \"task1\": load_imdb_dataset(Mode.train),\n",
    "    \"task2\": load_imdb_dataset(Mode.train),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e4a2201",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class InputExample:\n",
    "    uid: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TextClassificationExample(InputExample):\n",
    "    doc_tokens: List[str]\n",
    "    label: Optional[str] = None\n",
    "    positions: Optional[List[List[int]]] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class InputFeatures:\n",
    "    input_ids: List[int]\n",
    "    attention_mask: Optional[List[int]] = None\n",
    "    token_type_ids: Optional[List[int]] = None\n",
    "    positions: Optional[List[List[int]]] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TextClassificationFeatures(InputFeatures):\n",
    "    label: Optional[int] = None\n",
    "\n",
    "\n",
    "class Processor(ABC):\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer: Union[PreTrainedTokenizer, str],\n",
    "        max_seq_len: int,\n",
    "        label_list: List[str],\n",
    "        **kwargs,\n",
    "    ):\n",
    "        if isinstance(tokenizer, PreTrainedTokenizer):\n",
    "            self.tokenizer = tokenizer\n",
    "        else:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                tokenizer, **kwargs, use_fast=False\n",
    "            )\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.label_list = label_list\n",
    "\n",
    "    @abc.abstractclassmethod\n",
    "    def convert_examples_to_features(\n",
    "        self, examples: List[InputExample]\n",
    "    ) -> List[InputFeatures]:\n",
    "        \"\"\"Generate input features from examples\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @abc.abstractclassmethod\n",
    "    def features_to_dataset(\n",
    "        self, features: List[InputFeatures], mode: Union[str, Mode]\n",
    "    ) -> Dataset:\n",
    "        \"\"\"Get Pytorch Dataset object from list of input features\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "def is_whitespace(c):\n",
    "    if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7e433f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationProcessor(Processor):\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer: Union[PreTrainedTokenizer, str],\n",
    "        max_seq_len: int,\n",
    "        label_list: List[str],\n",
    "        multilabel: bool = False,\n",
    "        quotechar: str = '\"',\n",
    "        skiprows: int = 1,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            tokenizer=tokenizer,\n",
    "            max_seq_len=max_seq_len,\n",
    "            label_list=label_list,\n",
    "            **kwargs,\n",
    "        )\n",
    "        self.quotechar = quotechar\n",
    "        self.skiprows = skiprows\n",
    "        self.multilabel = multilabel\n",
    "\n",
    "    def get_examples(self, training_data: Dict) -> List[TextClassificationExample]:\n",
    "        \"\"\"\n",
    "        convert training data to bert examples\n",
    "        :param training_data: dict{\"text\": text, \"labels\", labels}\n",
    "        \"\"\"\n",
    "        assert len(training_data[\"text\"]) == len(training_data[\"labels\"]), (\n",
    "            f\"{len(training_data['text'])} text and {len(training_data['labels'])} labels\"\n",
    "        )\n",
    "        examples = []\n",
    "        is_contain_position = \"positions\" in training_data\n",
    "        for ii in tqdm(range(len(training_data[\"text\"]))):\n",
    "            label = training_data[\"labels\"][ii]\n",
    "            assert label in self.label_list, (\n",
    "                f\"Non exist label: '{label}' in label list: {self.label_list}.\"\n",
    "            )\n",
    "            context_text = training_data[\"text\"][ii]\n",
    "            # List of tokens of the doc.\n",
    "            doc_tokens: List[str] = []\n",
    "            # Mapping between position of the character to the word position.\n",
    "            char_to_word_offset = []\n",
    "            prev_is_whitespace = True\n",
    "            for cc in context_text:\n",
    "                if is_whitespace(cc):\n",
    "                    prev_is_whitespace = True\n",
    "                else:\n",
    "                    if prev_is_whitespace:\n",
    "                        doc_tokens.append(cc)\n",
    "                    else:\n",
    "                        doc_tokens[-1] += cc\n",
    "                    prev_is_whitespace = False\n",
    "                char_to_word_offset.append(len(doc_tokens) - 1)\n",
    "\n",
    "            if is_contain_position:\n",
    "                assert len(training_data[\"positions\"][ii]) == len(doc_tokens)\n",
    "                positions = training_data[\"positions\"][ii]\n",
    "            else:\n",
    "                positions = [[0, 0, 0, 0]] * len(doc_tokens)\n",
    "            examples.append(\n",
    "                TextClassificationExample(\n",
    "                    uid=f\"{ii}\", doc_tokens=doc_tokens, label=label, positions=positions\n",
    "                )\n",
    "            )\n",
    "        return examples\n",
    "\n",
    "    def _convert_example_to_feature(\n",
    "        self, example: TextClassificationExample, label_to_idx: Dict[str, int]\n",
    "    ) -> TextClassificationFeatures:\n",
    "        all_positions = []\n",
    "        all_doc_tokens = []\n",
    "        for ii, token in enumerate(example.doc_tokens):\n",
    "            sub_tokens = self.tokenizer.tokenize(token)\n",
    "            all_doc_tokens.extend(sub_tokens)\n",
    "            all_positions.extend([example.positions[ii]] * len(sub_tokens))\n",
    "        encoded_dict = self.tokenizer.encode_plus(\n",
    "            all_doc_tokens,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_seq_len,\n",
    "            return_token_type_ids=True,\n",
    "        )\n",
    "        position_pad = [0, 0, 0, 0]\n",
    "        if len(all_positions) <= self.max_seq_len - 2:\n",
    "            positions = [position_pad] + all_positions\n",
    "        else:\n",
    "            positions = [position_pad] + all_positions[: self.max_seq_len - 2]\n",
    "\n",
    "        if len(positions) < self.max_seq_len:\n",
    "            positions += [position_pad] * (self.max_seq_len - len(positions))\n",
    "        encoded_dict[\"positions\"] = positions\n",
    "        encoded_dict[\"label\"] = label_to_idx[example.label]\n",
    "        return TextClassificationFeatures(**encoded_dict)\n",
    "\n",
    "    def convert_examples_to_features(\n",
    "        self, examples: List[TextClassificationExample]\n",
    "    ) -> List[TextClassificationFeatures]:\n",
    "        \"\"\"Generate text classification features from examples\"\"\"\n",
    "        label_to_idx = {label: ii for ii, label in enumerate(self.label_list)}\n",
    "        features: List[TextClassificationFeatures] = []\n",
    "        for ii in tqdm(range(len(examples))):\n",
    "            features.append(\n",
    "                self._convert_example_to_feature(examples[ii], label_to_idx)\n",
    "            )\n",
    "        return features\n",
    "\n",
    "    def features_to_dataset(\n",
    "        self,\n",
    "        features: List[TextClassificationFeatures],\n",
    "        mode: Union[str, Mode] = Mode.train,\n",
    "    ) -> TensorDataset:\n",
    "        \"\"\"Get Pytorch Dataset object from list of classification features\"\"\"\n",
    "        if isinstance(mode, Mode):\n",
    "            mode = mode.value\n",
    "        dataset = [\n",
    "            torch.tensor([f.input_ids for f in features], dtype=torch.long),\n",
    "            torch.tensor([f.attention_mask for f in features], dtype=torch.long),\n",
    "            torch.tensor([f.token_type_ids for f in features], dtype=torch.long),\n",
    "            torch.tensor([f.positions for f in features], dtype=torch.long),\n",
    "            torch.tensor([f.label for f in features], dtype=torch.long),\n",
    "        ]\n",
    "\n",
    "        return TensorDataset(*dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e1ebf23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(data: Dict[str, Any], labels: List[str]) -> DataLoader:\n",
    "    dataprocessor = TextClassificationProcessor(\n",
    "        tokenizer=tokenizer, max_seq_len=512, label_list=labels\n",
    "    )\n",
    "    log.info(\"Load examples\")\n",
    "    examples = dataprocessor.get_examples(data)\n",
    "    log.info(\"Convert examples to features\")\n",
    "    features = dataprocessor.convert_examples_to_features(examples)\n",
    "    log.info(\"Construct dataset\")\n",
    "    dataset = dataprocessor.features_to_dataset(features)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=cfg.batch_size,\n",
    "        shuffle=False if cfg.num_devices > 1 else True,\n",
    "        sampler=(\n",
    "            DistributedSampler(dataset, shuffle=True) if cfg.num_devices > 1 else None\n",
    "        ),\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "42ad003d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 22:52:19,309 - __main__ - INFO - Load examples\n",
      "2025-04-15 22:52:19,309 - __main__ - INFO - Load examples\n",
      "100%|██████████| 25000/25000 [00:20<00:00, 1208.22it/s]\n",
      "2025-04-15 22:52:40,007 - __main__ - INFO - Convert examples to features\n",
      "2025-04-15 22:52:40,007 - __main__ - INFO - Convert examples to features\n",
      "100%|██████████| 25000/25000 [05:58<00:00, 69.77it/s] \n",
      "2025-04-15 22:58:38,323 - __main__ - INFO - Construct dataset\n",
      "2025-04-15 22:58:38,323 - __main__ - INFO - Construct dataset\n",
      "2025-04-15 22:58:47,665 - __main__ - INFO - Load examples\n",
      "2025-04-15 22:58:47,665 - __main__ - INFO - Load examples\n",
      "100%|██████████| 25000/25000 [00:13<00:00, 1847.51it/s]\n",
      "2025-04-15 22:59:01,208 - __main__ - INFO - Convert examples to features\n",
      "2025-04-15 22:59:01,208 - __main__ - INFO - Convert examples to features\n",
      "100%|██████████| 25000/25000 [06:40<00:00, 62.35it/s] \n",
      "2025-04-15 23:05:42,189 - __main__ - INFO - Construct dataset\n",
      "2025-04-15 23:05:42,189 - __main__ - INFO - Construct dataset\n"
     ]
    }
   ],
   "source": [
    "train_loaders = {\n",
    "    task_name: get_dataloader(*data)\n",
    "    for task_name, data in dataset_mapping.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "49b9001c",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_num_labels_mapping = {\n",
    "    task_name: len(data[1]) for task_name, data in dataset_mapping.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "67859e6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task1': 2, 'task2': 2}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_num_labels_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c896f17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for task_name, data in dataset_mapping.items():\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a4dddf3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "c:\\Users\\Admin\\OneDrive\\Documents\\DANC\\source_code\\base\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "pretrained_model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased', num_labels=list(task_num_labels_mapping.values())[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "07af9c1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([2, 2])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_num_labels_mapping.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7ea1f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "dd6c1335",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 23:38:47,722 - __main__ - INFO - Loading finetuned model for task task1\n",
      "2025-04-15 23:38:47,722 - __main__ - INFO - Loading finetuned model for task task1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-04-15 23:38:48,671 - __main__ - INFO - Loading finetuned model for task task2\n",
      "2025-04-15 23:38:48,671 - __main__ - INFO - Loading finetuned model for task task2\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "finetuned_models = dict()\n",
    "for task in cfg.tasks:\n",
    "    log.info(f\"Loading finetuned model for task {task}\")\n",
    "    finetuned_models[task] = BertForSequenceClassification.from_pretrained(\n",
    "        cfg.model, num_labels=task_num_labels_mapping[task]\n",
    "    )\n",
    "\n",
    "# Store the finetuned backbone.\n",
    "finetuned_backbone: Dict[str, nn.Module] = {\n",
    "    task: cast(BertLayer, model.bert)\n",
    "    for task, model in finetuned_models.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1df8ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_backbone = pretrained_model.bert\n",
    "model: nn.Module = task_arithmetic_merge_modules(\n",
    "    pretrained_backbone,\n",
    "    list(finetuned_backbone.values()),\n",
    "    scaling_coef=cfg.init_lambda,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f480b85c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=3072, out_features=768, bias=True)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cast(BertLayer, model.encoder.layer[1].output.dense)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
